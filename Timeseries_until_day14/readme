** # Time Series Analysis: ROC-AUC Comparison Across Models **
This repository presents a time series analysis of predictive models, comparing their ROC-AUC performance over time across train, test, and validation sets. We explore two machine learning models—Random Forest and Logistic Regression—and evaluate their performance as more data points (days) are added to the training.
#Analysis Overview
The dataset comprises time-series data, with key features recorded at specific intervals (days). We evaluate the performance of the models using ROC-AUC over seven specific time points: [0, 3, 5, 7, 10, 12, 14]. For each model, we analyze the performance on three datasets:

Training set: Used to train the model.
Test set: Held out during training to evaluate the model's performance.
Validation set: An external dataset used to validate the model's generalization capability on unseen data.

Models
The following models were used:

1. Random Forest (RF)
2. Logistic Regression (LR)

##Key Results
1. Random Forest Performance
Generalization: Random Forest performs well on both the test and validation sets, showing little to no overfitting. The train, test, and validation ROC-AUC values remain closely aligned across all days.
Consistency: The model’s performance improves as more days are added (up to day 10-12), with little gain beyond this point, suggesting the model has learned the key patterns by day 10.
Conclusion: Random Forest is a strong candidate for this problem, with excellent generalization and stability over time.
2. Logistic Regression Performance
Overfitting: Logistic Regression shows signs of overfitting—the train AUC is significantly higher than the test and validation AUCs, especially in the early days.
Test/Validation AUC: The model's performance improves with more data, but it struggles to match Random Forest’s test and validation performance. By day 10, the test and validation AUCs stabilize but remain lower than the training AUC.
Conclusion: Logistic Regression may not be the best choice for this problem due to its tendency to overfit and its lower generalization performance.


